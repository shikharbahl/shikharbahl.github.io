<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09220;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 400
    }

    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 600
    }

    heading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 600
    }

    papertitle {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 600
    }

    name {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      font-weight: 400
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/ri_logo.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Shikhar Bahl</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <!-- <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-146060570-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-146060570-1');
</script>
  <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css">
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Shikhar Bahl</name>
              </p>
              <p><p>Hi there! I am a Research Scientist and a member of the founding team at <a href="https://www.skild.ai/">Skild AI</a>. My goal is to build robots that can operate robustly in the wild. Previously, I did my PhD at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> of <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a> and <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>. I have also spent time at <a href="https://ai.facebook.com/">Meta AI</a> (FAIR) and at <a href="https://www.nvidia.com/en-us/research/robotics/">NVIDIA Robotics</a>.</p>

                <p> Prior to CMU, I did my undergrad at <a href="https://www.berkeley.edu/">UC Berkeley</a> in Applied Math and Computer Science, where I was affiliated with <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research</a> (BAIR) and worked with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> on problems in deep reinforcement learning and robotics.
              </p>


              <p> Feel free to contact me via email! You can reach me at shikharbahl24 -at- gmail dot com </p>


              <p align=center>
                <!-- <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="mailto:shikharbahl24@gmail.com">email</a> &nbsp/&nbsp
                <a href="data/shikharCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=bdHgGgEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/shikharbahl"> Twitter </a> &nbsp/&nbsp
                <a href="https://github.com/shikharbahl">GitHub</a>
              </p>
            </td>
            <td width="33%">
              <img src="images/rsz_shikhar_bahl_circle.png">
            </td>
          </tr>
        </table>

        
        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <ul id="news">
                    <!-- <li> <i>[May 2024]</i>&nbsp; <a href="https://robotics-transformer-x.github.io">OpenX Embodiment</a> awarded the Best Paper Award at ICRA 2024 -->
                    <li> <i>[May 2024]</i>&nbsp; Honored to be part of the <a href="https://sites.google.com/view/rsspioneers2024/participants?authuser=0">RSS Pioneers</a> 2024 cohort
                    <li> <i>[Feb 2024]</i>&nbsp; I will be starting as a Research Scientist and a member of the founding team at <a href="https://www.skild.ai/"> Skild AI</a>
                    <li> <i>[Feb 2024]</i>&nbsp; Defended my thesis, titled  "Watch, Practice, Improve: Towards In-the-wild Manipulation" </a>
                    <li> <i>[June 2022]</i>&nbsp; Our work on learning from human videos, <a href="https://robo-affordances.github.io/">VRB</a>, was covered by <a href="https://www.youtube.com/watch?v=HvEzgu8rWSE"> CBS </a>
                    <li> <i>[May 2023]</i>&nbsp; <a href="https://sear-rl.github.io/">SEAR </a>was selected as a long oral presentation @ ICML 2023
                    <li> <i>[Jan 2023]</i>&nbsp; I am a finalist for the <a href="https://research.nvidia.com/graduate-fellowships/2023">NVIDIA Graduate Fellowship</a>
                    <li> <i>[Nov 2022]</i>&nbsp; We are organizing the <a href="https://sites.google.com/view/roboadapt/">Workshop on Learning to Adapt and Improve in the Real World </a> @ CoRL 2022</li>
                    <li> <i>[Sep 2022]</i>&nbsp; Honored to be awarded the Uber Presidential Fellowship
                    <li> <i>[July 2022]</i>&nbsp; <a href="https://human2robot.github.io/">WHIRL</a> was covered by <a href="https://www.vox.com/recode/23280840/smart-home-automation-robots-chores" >Vox</a>, <a href="https://techcrunch.com/2022/07/18/teaching-home-robots-to-learn-by-watching-people/">TechCrunch</a>, <a href="https://www.asme.org/topics-resources/content/this-robot-learns-by-watching">ASME</a>, <a href="https://www.youtube.com/watch?v=S02s2FmXc4g&t=4s">Voice of America</a>, and <a href="https://human2robot.github.io/">others</a>
                    
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Research</heading>
                <p>
                  I am broadly interested in creating robust autonomous agents that operate with minimal or no human supervision, in the wild. My research focuses on learning for perception and robot control. Here is some of my work (representative papers are <span class="highlight">highlighted</span>): 
                </p>
              </td>
            </tr>
          </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">



          <tr>
            <td width="33%" valign="top" align="center"><a href="">
            <img src="images/hrp.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
            </a></td>
            <td width="67%" valign="top">
              <p><a href="" id="HRP">
              <heading>HRP: Human Affordances for Robotic Pre-Training</heading></a><br>
              Mohan Kumar Srirama, Sudeep Dasari*, Shikhar Bahl*, Abhinav Gupta*<br>
              RSS 2024
              </p>
          
              <div class="paper" id="hrp">
              <a href="">webpage</a> |
              <a href="javascript:toggleblock('hrp_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('hrp')" class="togglebib">bibtex</a> |
              <a href="">paper</a>
          
              <p align="justify"> <i style="display: none;" id="hrp_abs"></i></p>
          
          <pre xml:space="preserve" style="display:none;">
          @article{srirama2024human,
          title={HRP: Human Affordances for Robotic Pre-Training},
          author={,
          journal= {RSS},
          year={2024}
          }
          </pre>
              </div>
            </td>
        </tr>
          
          <tr>
            <td width="33%" valign="top" align="center"><a href="https://robotics-transformer-x.github.io/">
            <video autoplay="" loop="" muted="" src="images/openX.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
            </a></td>
            <td width="67%" valign="top">
              <p><a href="https://robotics-transformer-x.github.io/" id="SWIM">
              <heading>OpenX Embodiment: Robotic Learning Datasets and RT-X Models </heading></a><br>
          
              Open X-Embodiment Collaboration<br>
              ICRA 2024 &nbsp;<font color="red"><b>(Best Paper Award)</b></font>
              </p>
          
              <div class="paper" id="openX">
              <a href="https://robotics-transformer-x.github.io/">webpage</a> |
              <a href="https://arxiv.org/pdf/2310.08864.pdf">pdf</a> |
              <a href="javascript:toggleblock('openX-abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('openX')" class="togglebib">bibtex</a> |
             
              <p align="justify"> <i style="display: none;" id="openX-abs">Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.
                
              </i></p>
          
              <pre xml:space="preserve" style="display:none;">
                @misc{open_x_embodiment_rt_x_2023,
                  title={Open {X-E}mbodiment: Robotic
                  Learning Datasets and {RT-X} Models},
                  author = {Open X-Embodiment Collaboration },
                  howpublished  = {\url{https://arxiv.org/abs/2310.08864}},
                  year = {2023},
                  }
              </pre>
              </div>
            </td>
          </tr>

          <tr>
              <td width="33%" valign="top" align="center"><a href="https://dexterous-finetuning.github.io/">
              <video autoplay loop muted src="images/pour_cup.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
              </a></td>
              <td width="67%" valign="top">
                <p><a href="https://dexterous-finetuning.github.io/" id="DEFT">
                <heading>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies</heading></a><br>
                Aditya Kannan*, Kenneth Shaw*, Shikhar Bahl, Pragna Mannam, Deepak Pathak<br>
                CoRL 2023
                </p>
            
                <div class="paper" id="deft">
                <a href="https://dexterous-finetuning.github.io/">webpage</a> |
                <a href="javascript:toggleblock('deft_abs')">abstract</a> |
                <a shape="rect" href="javascript:togglebib('deft')" class="togglebib">bibtex</a> |
                <a href="https://openreview.net/pdf?id=wH23nZpVTF6">CoRL</a>
            
                <p align="justify"> <i style="display: none;" id="deft_abs">Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. Although, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation.</i></p>
            
            <pre xml:space="preserve" style="display:none;">
            @article{kannan2023deft,
            title={DEFT: Dexterous Fine-Tuning for Real-World Hand Policies},
            author={Kannan, Aditya* and Shaw, Kenneth* and Bahl, Shikhar and Mannam, Pragna and Pathak, Deepak},
            journal= {CoRL},
            year={2023}
            }
            </pre>
                </div>
              </td>
          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://video-dex.github.io">
            <video autoplay loop muted src="images/videodex.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
            </a></td>
            <td width="67%" valign="top">
              <heading>Learning Dexterity from Human Hand Motion in Internet Videos</heading></a><br>
              Kenneth Shaw*, Shikhar Bahl*, Aravind Sivakumar, Aditya Kannan, Deepak Pathak<br>
              IJRR 2023 Special Issue
              </p>
              <div class="paper" id="ijrr_human">
              <a href="javascript:toggleblock('ijrr_human_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('ijrr_human')" class="togglebib">bibtex</a>
        
              <p align="justify"> <i style="display: none;" id="ijrr_human_abs">To build general robotic agents that can operate in many environments, it is often useful for robots to collect experience in the real world.  However, unguided experience collection is often not feasible due to safety, time, and hardware restrictions.  We thus propose leveraging the next best thing as real world experience: videos of humans using their hands. To utilize these videos, we develop a method that retargets any 1st person or 3rd person video of human hands and arms into the robot hand and arm trajectories.   While retargeting is a difficult problem, our key insight is to rely on only internet human hand video to train it.  We use this method to present results in two areas:  First, we build a system that enables any human to control a robot hand and arm, simply by demonstrating motions with their own hand. The robot observes the human operator via a single RGB camera and imitates their actions in real-time.  This enables the robot to collect real-world experience safely using supervision.  Second, we retarget in-the-wild human internet video into task-conditioned pseudo-robot trajectories to use as artificial robot experience.  This learning algorithm leverages action priors from human hand actions, visual features from the images, and physical priors from dynamical systems to pretrain typical human behavior for a particular robot task.  We show that by leveraging internet human hand experience, we need fewer robot demonstrations compared to many other methods.</i></p>
        
          <pre xml:space="preserve" style="display:none;">
            @article{shaw_internetvideos,
              title={Learning Dexterity from Human Hand Motion in Internet Videos},
              author={Shaw, Kenneth and Bahl,
              Shikhar and Sivakumar, Aravind and Kannan, Aditya and Pathak, Deepak},
              journal= {IJRR},
              year={2022}
            }
          </pre>
              </div>
            </td>
          </tr>

          
          <tr>
            <td width="33%" valign="top" align="center"><a href="https://human-world-model.github.io/">
            <video autoplay loop muted src="images/swim.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
            </a></td>
            <td width="67%" valign="top">
              <p><a href="https://human-world-model.github.io/" id="SWIM">
              <heading>Structured World Models from Human Videos</heading></a><br>
              Russell Mendonca*, Shikhar Bahl*, Deepak Pathak<br>
              RSS 2023 &nbsp;<font color="red"><b>(Invited to IJRR Special Issue)</b></font>
              </p>
          
              <div class="paper" id="swim">
              <a href="https://human-world-model.github.io/">webpage</a> |
              <a href="javascript:toggleblock('swim_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('swim')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/2308.10901">arXiv</a>
          
               <p align="justify"> <i style="display: none;" id="swim_abs">We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction.</i></p>
          
           <pre xml:space="preserve" style="display:none;">
            @article{mendonca23swim,
            title={Structured World Models
            from Human Videos},
            author={Mendonca, Russell and
            Bahl, Shikhar and Pathak, Deepak},
            journal={RSS},
            year={2023},
          }
          </pre>
          
              </div>
            </td>
          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://sear-rl.github.io/">
              <img src="images/sear.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:0px solid black">
              </a></td>
            <td width="67%" valign="top">
              <p><a href="https://sear-rl.github.io/" id="SEAR">
              <heading>Efficient RL via Disentangled Environment and <br> Agent Representations</heading></a><br>
          Kevin Gmelin*, Shikhar Bahl*, Russell Mendonca, Deepak Pathak<br>
              ICML 2023 &nbsp;<font color="red"><b>(Oral)</b></font> <br>
              </p>
          
              <div class="paper" id="sear">
              <a href="https://sear-rl.github.io/">webpage</a> |
              <a href="javascript:toggleblock('sear_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('sear')" class="togglebib">bibtex</a> |
              <a href="https://openreview.net/pdf?id=kWS8mpioS9">pdf</a>
          
               <p align="justify"> <i style="display: none;" id="sear_abs">Agents that are aware of the separation between the environments and themselves can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, which is often inexpensive to obtain, such as its shape or mask. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, SEAR (Structured Environment-Agent Representations), outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots.</i></p>
          
          <pre xml:space="preserve" style="display:none;">
          @article{Gmelin2023sear,
          title={Efficient RL via Disentangled
          Environment and Agent Representations},
          author={Gmelin, Kevin and Bahl, Shikhar
          and Mendonca, Russell and Pathak, Deepak},
          journal={ICML},
          year={2023}
          }
              </pre>
              </div>
            </td>
          </tr>

          <tr>
            <tr bgcolor="#ffffd0">
            <td width="33%" valign="top" align="center"><a href="https://robo-affordances.github.io/">
            <video autoplay="" loop="" muted="" src="images/vrb_website_small.mp4" alt="sym" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
            </a></td>
            <td width="67%" valign="top">
              <p><a href="https://robo-explorer.github.io/" id="VRB">
              <heading>Affordances from Human Videos as a Versatile Representation for Robotics</heading></a>
              <br>Shikhar Bahl*, Russell Mendonca*, Lili Chen, Unnat Jain, Deepak Pathak<br>
              CVPR 2023
              </p>
          
              <div class="paper" id="VRB">
              <a href="https://robo-affordances.github.io/">webpage</a> |
              <a href="javascript:toggleblock('vrb-abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('VRB')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/2304.08488">arXiv</a> | 
              <a href="https://github.com/shikharbahl/vrb">code</a>
             
          
              <p align="justify"> <i style="display: none;" id="VRB-abs">Building a robot that can understand and learn to interact by watching humans has inspired several vision problems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact. The structure of these behavioral affordances directly enables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learning, exploration, goal-conditioned learning, and action parameterization for reinforcement learning. We show the efficacy of our approach, which we call Vision-Robotics Bridge (VRB) as we aim to seamlessly integrate computer vision techniques with robotic manipulation, across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild.</i></p>
          
          <pre xml:space="preserve" style="display:none;">
            @inproceedings{bahl2023affordances,
              title={Affordances from Human Videos as a Versatile Representation for Robotics},
              author={Bahl, Shikhar and Mendonca, Russell and Chen, Lili and Jain, Unnat and Pathak, Deepak},
              journal={CVPR},
              year={2023}
            }
          </pre>
              </div>
            </td>
          </tr>
        </tr>
          

          
	<tr>
  <td width="33%" valign="top" align="center"><a href="https://robo-explorer.github.io/">
  <video autoplay="" loop="" muted="" src="images/alan.mp4" alt="sym" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://robo-explorer.github.io/" id="ALAN">
    <heading>ALAN : Autonomously Exploring Robotic Agents
      in the Real World</heading></a>
    <br>Russell Mendonca, Shikhar Bahl, Deepak Pathak<br>
    ICRA 2023 
    </p>

    <div class="paper" id="alan">
    <a href="https://robo-explorer.github.io/">webpage</a> |
    <a href="javascript:toggleblock('alan-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('alan')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2302.06604">arXiv</a> | 
   

    <p align="justify"> <i style="display: none;" id="alan-abs">Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform many tasks in the real world with little training and interaction time. This is enabled by measuring environment change, which reflects object movement and ignores changes in the robot position. We use this metric directly as an environment-centric signal, and also maximize the uncertainty of predicted environment change, which provides agent-centric exploration signal. We evaluate our approach on two different real-world play kitchen settings, enabling a robot to efficiently explore and discover manipulation skills, and perform tasks specified via goal images.</i></p>

<pre xml:space="preserve" style="display:none;">
  @article{mendonca2023alan,
    author = {Mendonca, Russell and
    Bahl, Shikhar and
    Pathak, Deepak},
    title  = {ALAN : Autonomously Exploring Robotic Agents in the Real World},
    journal= {ICRA},
    year   = {2023}
  }
</pre>
    </div>
  </td>
</tr>



  <tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://video-dex.github.io">
  <video autoplay loop muted src="images/videodex.mp4" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://video-dex.github.io" id="VIDEODEX">
    <heading>VideoDex: Learning Dexterity from Internet Videos</heading></a><br>
    Kenneth Shaw*, Shikhar Bahl*, Deepak Pathak<br>
    CoRL 2022
    </p>

    <div class="paper" id="videodex">
    <a href="https://video-dex.github.io">webpage</a> |
    <a href="javascript:toggleblock('videodex_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('videodex')" class="togglebib">bibtex</a> |
    <a href="https://video-dex.github.io">arXiv</a> |
    <a href="https://video-dex.github.io">demo</a>

    <p align="justify"> <i style="display: none;" id="videodex_abs">To build general robotic agents that can operate in many environments, it is often imperative for the robot to collect experience in the real world.  However, this is often not feasible due to safety, time and hardware restrictions.  We thus propose leveraging the next best thing as real world experience: internet videos of humans using their hands.  Visual priors, such as visual features, are often learned from videos, but we believe that more information from videos can be utilized as a stronger prior.  We build a learning algorithm, VideoDex, that leverages visual, action and physical priors from human video datasets to guide robot behavior.  These action and physical priors in the neural network dictate the typical human behavior for a particular robot task.   We test our approach on a robot arm and dexterous hand based system and show strong results on many different manipulation tasks, outperforming various state-of-the-art methods.</i></p>

<pre xml:space="preserve" style="display:none;">
  @article{videodex,
    title={VideoDex: Learning Dexterity
    from Internet Videos},
    author={Shaw, Kenneth and Bahl,
    Shikhar and Pathak, Deepak},
    journal= {CoRL},
    year={2022}
  }
</pre>
    </div>
  </td>
</tr> 


        <tr bgcolor="#ffffd0">
            <td width="33%" valign="top" align="center"><a href="https://human2robot.github.io/">
            <!-- <video autoplay loop muted src="images/hndp.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video> -->
            <img src="images/whirl.gif" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
            </a></td>
            <td width="67%" valign="top">
              <p><a href="https://human2robot.github.io/" id="WHIRL">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Human-to-Robot Imitation in the Wild</heading></a><br>
              <strong>Shikhar Bahl</strong>, Abhinav Gupta*, Deepak Pathak*<br>
              RSS 2022
              </p>
        
              <div class="paper" id="whirl">
              <a href="https://human2robot.github.io/">webpage</a> |
              <a href="https://human2robot.github.io/resources/paper.pdf">pdf</a> |
              <a href="javascript:toggleblock('whirl_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('whirl')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/2207.09450">arXiv</a> |
              <a href="https://youtu.be/pIbHOuvuCl8">videos</a> |
              <a href="https://youtu.be/tSdksoVUzgQ">talk</a>
        
              <p align="justify"> <i id="whirl_abs">We approach the problem of learning by watching humans in the wild. While traditional approaches in Imitation and Reinforcement Learning are promising for learning in the real world, they are either sample inefficient or are constrained to lab settings. Meanwhile, there has been a lot of success in processing passive, unstructured human data. We propose tackling this problem via an efficient one-shot robot learning algorithm, centered around learning from a third person perspective. We call our method WHIRL: In the Wild Human-Imitated Robot Learning. In WHIRL, we aim to use human videos to extract a prior over the intent of the demonstrator, and use this to initialize our agent's policy. We introduce an efficient real-world policy learning scheme, that improves over the human prior using interactions. Our key contributions are a simple sampling-based policy optimization approach, a novel objective function for aligning human and robot videos as well as an exploration method to boost sample efficiency. We show, one-shot, generalization and success in real world settings, including 20 different manipulation tasks in the wild.</i></p>
        
        <pre xml:space="preserve">
        @article{bahl2022human,
          author = {Bahl, Shikhar and
          Gupta, Abhinav and Pathak, Deepak},
          title  = {Human-to-Robot Imitation in the Wild},
          journal= {RSS},
          year   = {2022}
        }
        </pre>
              </div>
            </td>
          </tr>



          <tr>
            <td width="33%" valign="top" align="center"><a href="https://rb2.info/">
            <img src="images/rb2.jpeg" alt="sym" width="88%" style="padding:0px;border-radius:15px;border:0  px solid gray">
            </a></td>
            <td width="67%" valign="top">
            <p><a href="https://rb2.info/" id="RB2">
            <heading>RB2: Robotic Manipulation Benchmarking with a Twist</heading></a><br>
    Sudeep Dasari, Jianren Wang, Joyce Hong, <strong>Shikhar Bahl</strong>, Abitha Thankaraj, Karanbir Chahal, Berk Calli, Saurabh Gupta, David Held, Lerrel Pinto, Deepak Pathak, Vikash Kumar, Abhinav Gupta<br>
            NeurIPS 2021<br>
            (Datasets and Benchmark)
            </p>
            
            <div class="paper" id="rb2">
            <a href="https://rb2.info/">webpage</a> |
            <a href="https://openreview.net/pdf?id=e82_BlJL43M">pdf</a> |
            <a href="javascript:toggleblock('rb2_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('rb2')" class="togglebib">bibtex</a> |
            <!-- <a href="https://arxiv.org/abs/2111.13119">arXiv</a> | -->
            <a href="https://github.com/SudeepDasari/robot_baselines">code</a>
            
            <p align="justify"> <i id="rb2_abs">Benchmarks offer a scientific way to compare algorithms using objective performance metrics. Good benchmarks have two features: (a) they should be widely useful for many research groups; (b) and they should produce reproducible findings. In robotic manipulation research, there is a trade-off between reproducibility and broad accessibility. If the benchmark is kept restrictive (fixed hardware, objects), the numbers are reproducible but the setup becomes less general. On the other hand, a benchmark could be a loose set of protocols (e.g. YCB object set) but the underlying variation in setups make the results non-reproducible. In this paper, we re-imagine benchmarking for robotic manipulation as state-of-the-art algorithmic implementations, alongside the usual set of tasks and experimental protocols. The added baseline implementations will provide a way to easily recreate SOTA numbers in a new local robotic setup, thus providing credible relative rankings between existing approaches and new work. However, these 'local rankings' could vary between different setups. To resolve this issue, we build a mechanism for pooling experimental data between labs, and thus we establish a single global ranking for existing (and proposed) SOTA algorithms. Our benchmark, called Ranking-Based Robotics Benchmark (RB2), is evaluated on tasks that are inspired from clinically validated Southampton Hand Assessment Procedures. Our benchmark was run across two different labs and reveals several surprising findings. For example, extremely simple baselines like open-loop behavior cloning, outperform more complicated models (e.g. closed loop, RNN, Offline-RL, etc.) that are preferred by the field. We hope our fellow researchers will use RB2 to improve their research's quality and rigor.</i></p>
            
            <pre xml:space="preserve">
            @inproceedings{dasari2021rb2,
              title={RB2: Robotic Manipulation
              Benchmarking with a Twist},
              author={Dasari, Sudeep and
              Wang, Jianren and Hong, Joyce and
              Bahl, Shikhar and Lin, Yixin and
              Wang, Austin S and Thankaraj, Abitha
              and Chahal, Karanbir Singh and
              Calli, Berk and Gupta, Saurabh
              and others},
              booktitle={Thirty-fifth Conference
              on Neural Information Processing
              Systems Datasets and Benchmarks
              Track (Round 2)},
              year={2021}
            }
            </pre>
            </div>
            </td>
            </tr>

          <tr bgcolor="#ffffd0">
            <td width="33%" valign="top" align="center"><a href="https://shikharbahl.github.io/hierarchical-ndps/">
            <!-- <video autoplay loop muted src="images/hndp.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video> -->
            <img src="images/hndp.gif" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
            </a></td>
            <td width="67%" valign="top">
              <p><a href="https://shikharbahl.github.io/hierarchical-ndps/" id="HNDP">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Hierarchical Neural Dynamic Policies</heading></a><br>
              Shikhar Bahl, Abhinav Gupta, Deepak Pathak<br>
              RSS 2021 &nbsp;<font color="red"><b>(Invited to Autonomous Robots Special Issue)</b></font>
              </p>
        
              <div class="paper" id="hndp">
              <a href="https://shikharbahl.github.io/hierarchical-ndps/">webpage</a> |
              <a href="https://arxiv.org/pdf/2107.05627.pdf">pdf</a> |
              <a href="javascript:toggleblock('hndp_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('hndp')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/2107.05627">arXiv</a> |
              <!-- <a href="https://github.com/buoyancy99/unsup-3d-keypoints">code</a> | -->
              <a href="https://youtu.be/VqCprLrvSsg">talk video</a>
        
              <p align="justify"> <i id="hndp_abs">We tackle the problem of generalization to unseen configurations for dynamic tasks in the real world while learning from high-dimensional image input. The family of nonlinear dynamical system-based methods have successfully demonstrated dynamic robot behaviors but have difficulty in generalizing to unseen configurations as well as learning from image inputs. Recent works approach this issue by using deep network policies and reparameterize actions to embed the structure of dynamical systems but still struggle in domains with diverse configurations of image goals, and hence, find it difficult to generalize. In this paper, we address this dichotomy by leveraging embedding the structure of dynamical systems in a hierarchical deep policy learning framework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of fitting deep dynamical systems to diverse data directly, H-NDPs form a curriculum by learning local dynamical system-based policies on small regions in state-space and then distill them into a global dynamical system-based policy that operates only from high-dimensional images. H-NDPs additionally provide smooth trajectories, a strong safety benefit in the real world. We perform extensive experiments on dynamic tasks both in the real world (digit writing, scooping, and pouring) and simulation (catching, throwing, picking). We show that H-NDPs are easily integrated with both imitation as well as reinforcement learning setups and achieve state-of-the-art results.</i></p>
        
        <pre xml:space="preserve">
        @article{bahl2021hndp,
          author = {Bahl, Shikhar and
          Gupta, Abhinav and Pathak, Deepak},
          title  = {Hierarchical Neural
          Dynamic Policies},
          journal= {RSS},
          year   = {2021}
        }
        </pre>
              </div>
            </td>
          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><a href="https://shikharbahl.github.io/neural-dynamic-policies/"><img src="images/neurips20_ndp.jpeg" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:2px;padding-bottom:2px;padding-left:2px;padding-right:2px;border-radius:8px;"></a></td>
            <td width="67%" valign="top">
              <p><a href="https://shikharbahl.github.io/neural-dynamic-policies/" id="NEURIPS20_NDP">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Neural Dynamic Policies for End-to-End Sensorimotor Learning</heading></a><br>
              Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, Deepak Pathak<br>
              NeurIPS 2020 &nbsp;<font color="red"><b>(Spotlight)</b></font>
              </p>
        
              <div class="paper" id="neurips20_ndp">
              <a href="https://shikharbahl.github.io/neural-dynamic-policies/">webpage</a> |
              <a href="https://shikharbahl.github.io/neural-dynamic-policies/resources/ndp.pdf">pdf</a> |
              <a href="javascript:toggleblock('neurips20_ndp_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('neurips20_ndp')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/2012.02788">arXiv</a> |
              <a href="https://github.com/shikharbahl/neural-dynamic-policies/">code</a> |
              <a href="https://youtu.be/vQOHx8u_GWA">demo</a> |
              <a href="https://youtu.be/1VqiL1nF6sI">spotlight talk</a>
        
              <p align="justify"> <i id="neurips20_ndp_abs">The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decision at each point in training, and hence, limit the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or deep reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed dynamics structure into deep neural network-based policies by reparameterizing action spaces with differential equations. We propose Neural Dynamic Policies (NDPs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where action represents the raw control space. The embedded structure allow us to perform end-to-end policy learning under both reinforcement and imitation learning setups. We show that NDPs achieve better or comparable performance to state-of-the-art approaches on many robotic control tasks using both reward-based training and demonstrations.</i></p>
        
        <pre xml:space="preserve">
        @inproceedings{bahl2020ndp,
          Author = {Bahl, Shikhar and
          Mukadam, Mustafa and
          Gupta, Abhinav and Pathak, Deepak},
          Title = {Neural Dynamic Policies
          for End-to-End Sensorimotor Learning},
          Booktitle = {NeurIPS},
          Year = {2020}
        }
        </pre>
              </div>
            </td>
          </tr>



          <tr>
            <td width="33%" valign="top" align="center"><a href="https://sites.google.com/view/skew-fit"><img src="images/skewfit_new.png" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:2px;padding-bottom:2px;padding-left:2px;padding-right:2px;border-radius:8px;"></a></td>
            <td width="67%" valign="top">
              <p><a href="https://sites.google.com/view/skew-fit" id="SKEWFIT">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Skew-Fit: State-Covering Self-Supervised Reinforcement Learning</heading></a><br>
              Vitchyr H. Pong*, Murtaza Dalal*, Steven Lin*, Ashvin Nair, Shikhar Bahl, Sergey Levine<br>
              ICML 2020
              </p>
        
              <div class="paper" id="skewfit">
              <a href="https://sites.google.com/view/skew-fit">webpage</a> |
              <a href="https://arxiv.org/pdf/1903.03698.pdf">pdf</a> |
              <a href="javascript:toggleblock('skewfit_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('skewfit')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/1903.03698">arXiv</a> |
              <a href="https://github.com/rail-berkeley/rlkit/tree/master/examples/skewfit">code</a> |
              <a href="https://youtu.be/DWSZHEvZO4o">video</a> |
        
              <p align="justify"> <i id="skewfit_abs">Reinforcement learning can enable an agent to acquire a large repertoire of skills. However, each new skill requires a manually-designed reward function, which typically requires considerable manual effort and engineering. Self-supervised goal setting has the potential to automate this process, enabling an agent to propose its own goals and acquire skills that achieve these goals. However, such methods typically rely on manually-designed goal distributions or heuristics to encourage the agent to explore a wide range of states. In this work, we propose a formal objective for exploration when training an autonomous goal-reaching policy that maximizes state coverage, and show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance. We present an algorithm called Skew-Fit for learning such a maximum-entropy goal distribution, and show that our method converges to a uniform distribution over the set of possible states, even when we do not know this set beforehand. When combined with existing goal-conditioned reinforcement learning algorithms, we show that Skew-Fit allows self-supervised agents to autonomously explore their entire state space faster than prior work, across a variety of simulated and real robotic tasks.</i></p>
        
        <pre xml:space="preserve">
        @inproceedings{bahl2020ndp,
          title={Skew-fit: State-covering self-supervised reinforcement learning},
          author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
          journal={ICML},
          year={2020}
        }
        </pre>
              </div>
            </td>
          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><a href="https://industrial-insertion-rl.github.io/">
              <img src="images/natural_rew.gif" alt="sym" width="62.5%" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
            <td width="67%" valign="top">
              <p><a href="https://industrial-insertion-rl.github.io/" id="NATURAL_REW">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards</heading></a><br>
              Gerrit Schoettler*, Ashvin Nair*, Jianlan Luo, Shikhar Bahl, Juan aparicio Ojea, Eugen Solowjow, Sergey Levine<br>
              IROS 2020
              </p>
        
              <div class="paper" id="natural_rew">
              <a href="https://industrial-insertion-rl.github.io/">webpage</a> |
              <a href="https://arxiv.org/pdf/1906.05841.pdf">pdf</a> |
              <a href="javascript:toggleblock('natural_rew_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('natural_rew')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/1906.05841">arXiv</a> |
              <a href="https://youtu.be/t37YS7XA6VU">video</a>
        
              <p align="justify"> <i id="natural_rew_abs">We consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.</i></p>
        
        <pre xml:space="preserve">
          @article{nair2020contextual,
            title={Deep reinforcement learning for industrial insertion tasks with visual inputs and natural rewards},
            author={Schoettler, Gerrit and Nair, Ashvin and Luo, Jianlan and Bahl, Shikhar and Ojea, Juan Aparicio and Solowjow, Eugen and Levine, Sergey},
            booktitle={IROS},
            year={2020},
          }
        </pre>
              </div>
            </td>
          </tr>


      



          <tr>
            <td width="33%" valign="top" align="center"><a href="https://ccrig.github.io/">
              <img src="images/ccrig.gif" alt="sym" height="50%" style="border: 1px solid #bbb;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
            <td width="67%" valign="top">
              <p><a href="https://ccrig.github.io/" id="CCRIG">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Contextual Imagined Goals for Self-Supervised Robotic Learning</heading></a><br>
              Ashvin Nair*, Shikhar Bahl*, Alexander Khazatsky*, Vitchyr H. Pong, Glen Berseth, Sergey Levine<br>
              CoRL 2019
              </p>
        
              <div class="paper" id="ccrig">
              <a href="https://ccrig.github.io/">webpage</a> |
              <a href="http://proceedings.mlr.press/v100/nair20a/nair20a.pdf">pdf</a> |
              <a href="javascript:toggleblock('ccrig_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('ccrig')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/1910.11670">arXiv</a> |
              <a href="https://github.com/anair13/rlkit/tree/ccrig/examples/ccrig">code</a> |
              <a href="https://drive.google.com/file/d/1xwbWOjX9e1M5ZCMBp5PMz0WHgUJ1qJNm/view">data</a> |
              <a href="https://youtu.be/Cqp_phACou0">video</a>
        
              <p align="justify"> <i id="ccrig_abs">We propose a conditional goal-setting model that aims to only propose goals that are feasible reachable from the robot's current state, and demonstrate that this enables self-supervised goal-conditioned learning with raw image observations both in varied simulated environments and a real-world pushing task..</i></p>
        
        <pre xml:space="preserve">
          @article{nair2020contextual,
            title={Contextual imagined goals for self-supervised robotic learning},
            author={Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
            booktitle={CoRL},
            year={2020},
          }
        </pre>
              </div>
            </td>
          </tr>



          <tr bgcolor="#ffffd0">
            <td width="33%" valign="top" align="center"><a href="https://residualrl.github.io/"><img src="images/residual.png" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:2px;padding-bottom:2px;padding-left:2px;padding-right:2px;border-radius:8px;"></a></td>
            <td width="67%" valign="top">
              <p><a href="https://residualrl.github.io/" id="RESIDUAL">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Residual Reinforcement Learning for Robot Control</heading></a><br>
              Tobias Johannink*, Shikhar Bahl*, Ashvin Nair*, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea,  Eugen Solowjow, Sergey Levine<br>
              ICRA 2019
              </p>
        
              <div class="paper" id="resiudal">
              <a href="https://residualrl.github.io/">webpage</a> |
              <a href="https://arxiv.org/pdf/1812.03201.pdf">pdf</a> |
              <a href="javascript:toggleblock('resiudal_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('residual')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/1812.03201">arXiv</a> |
              <a href="https://www.youtube.com/watch?v=RRllUka-r2A">video</a>
        

              <p align="justify"> <i id="residual_abs">Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects</i></p>
          <pre xml:space="preserve">
            @inproceedings{johannink2019residual,
              title={Residual reinforcement learning for robot control},
              author={Johannink, Tobias and Bahl, Shikhar and Nair, Ashvin and Luo, Jianlan and Kumar, Avinash and Loskyll, Matthias and Ojea, Juan Aparicio and Solowjow, Eugen and Levine, Sergey},
              booktitle={ICRA},
              year={2019},
            }
            }
          </pre>
            </div>
            </td>
          </tr>



          <tr>
            <td width="33%" valign="top" align="center"><a href="https://sites.google.com/site/visualrlwithimaginedgoals/"><img src="images/rig_2.png" alt="sym" width="90%" style="border: 1px solid #bbb;padding-top:2px;padding-bottom:2px;padding-left:2px;padding-right:2px;border-radius:8px;"></a></td>
            <td width="67%" valign="top">
              <p><a href="https://sites.google.com/site/visualrlwithimaginedgoals/" id="RIG">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Visual Reinforcement Learning with Imagined Goals</heading></a><br>
              Ashvin Nair*, Vitchyr H. Pong*, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine<br>
              NeurIPS 2018 &nbsp;<font color="red"><b>(Spotlight)</b></font>
              </p>
        
              <div class="paper" id="rig">
              <a href="https://sites.google.com/site/visualrlwithimaginedgoals/">webpage</a> |
              <a href="https://arxiv.org/pdf/1807.04742.pdf">pdf</a> |
              <a href="javascript:toggleblock('rig_abs')">abstract</a> |
              <a shape="rect" href="javascript:togglebib('rig')" class="togglebib">bibtex</a> |
              <a href="https://arxiv.org/abs/1807.04742">arXiv</a> |
              <a href="https://github.com/rail-berkeley/rlkit">code</a> |
              <a href="https://bair.berkeley.edu/blog/2018/09/06/rig/">blog</a> |
              <a href="https://sites.google.com/site/visualrlwithimaginedgoals/">videos</a>
        
              <p align="justify"> <i id="rig_abs">For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.</i></p>
        
        <pre xml:space="preserve">
          @article{nair2018visual,
            title={Visual reinforcement learning with imagined goals},
            author={Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
            journal={NeurIPS},
            year={2018}
          }
        </pre>
              </div>
            </td>
          </tr>








        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/SparseGraphSenators.png" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EECS127/">
                  <papertitle>EECS127 - Fall 2018 (uGSI)</papertitle>
                </a>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                    Website template from this <a href="https://github.com/jonbarron/jonbarron_website"><strong>repo</strong></a> and this <a href="https://www.cs.cmu.edu/~dpathak/"><strong>webpage</strong></a>!
                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('hndp_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('rb2_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('neurips20_ndp_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('rig_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('ccrig_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('residual_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('natural_rew_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('skewfit_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('whirl_abs');
  </script>
  <!-- <script xml:space="preserve" language="JavaScript">
    hideblock('skewfit_abs');
  </script> -->

  <!-- <script xml:space="preserve" language="JavaScript">
    hideblock('natural_reward_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('ccrig_abs');
  </script> -->
</body>

</html>

